{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab05ceb9-c258-4a22-bafe-1aa335d40b8f",
   "metadata": {},
   "source": [
    "# Process, analyze, and summarize data with Spark and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794cc26-aa69-4047-a690-2a342ebb524a",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Did you know that a billion records are processed daily in PySpark by companies worldwide? As big data is on the rise, you’ll need tools like PySpark to process massive amounts of data.\n",
    "\n",
    "This guided project was designed to introduce data analysts and data science beginners to data analysis in PySpark. This 2-hour project course teaches you how to create a PySpark environment, explore and clean large data, aggregate and summarize data, and visualize data using real-life examples. By the end of this guided project, you’ll create a Jupyter Notebook that processes, analyzes, and summarizes data using PySpark. By working on hands-on tasks, you will gain a solid knowledge of data aggregation and summarization with PySpark, helping you acquire job-ready skills. \n",
    "\n",
    "You don’t need any experience in PySpark, but knowledge of Python is essential to succeeding in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156abaa0-8a56-4155-b441-18af5eeeacae",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This project demonstrates how to process and analyze large datasets using PySpark, focusing on employees data. Tasks include data loading, cleaning, exploration, and aggregation, culminating in insights on employee salaries and demographics.\n",
    "\n",
    "### About the Dataset\n",
    "**employees.csv**: Contains employee details like employee numbers, names, birth dates, hire dates, etc.\n",
    "\n",
    "**updated_salaries.csv**: Contains salary details for each employee, including salary amounts and dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd3c17-248d-4808-956f-21c01eff8e2d",
   "metadata": {},
   "source": [
    "# Task One: Set up and overview of the project\n",
    "In this task, you will get an overview of the project and set up the PySpark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5cd815f-63b3-4e74-9b53-35237b7383da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, avg, max, min, countDistinct, sum, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e15426d-f29a-44b3-9335-9e64b31a7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up PySpark environment\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6ea312-498a-43f1-b68e-12d75abb805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 13:49:24 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-5.local resolves to a loopback address: 127.0.0.1; using 192.168.3.155 instead (on interface en0)\n",
      "25/04/14 13:49:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 13:49:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.155:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Foundations Course</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x121c97f90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize the SparkSession\n",
    "# appName is the name of the application\n",
    "# getOrCreate() creates a new session or retrieves an existing one\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Foundations Course\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "## Verify that SparkSession is created\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bfd46-805d-466f-b6f7-9e0db5113efa",
   "metadata": {},
   "source": [
    "# Task Two: Load the data\n",
    "In this task, you will load the employees.csv and updated_salaries.csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3021b17b-9829-4cdb-921e-3c66d7744071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee data loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Load the employees.csv dataset\n",
    "# header=True means the first row is the header \n",
    "# inferSchema=True means Spark will try to infer the data types of the columns\n",
    "try: # handle error if file not found    \n",
    "    emp_df = spark.read.csv('./employees.csv', header=True, inferSchema=True) \n",
    "    print('Employee data loaded successfully')\n",
    "except Exception as e:\n",
    "    print(f'Error loading employee data: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6f5c39-3b6d-48f7-8196-64dd2f2216aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary data loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Load the updated_salaries.csv dataset\n",
    "try:\n",
    "    sal_df = spark.read.format('csv') \\\n",
    "        .option('header', 'true')\\\n",
    "        .option('inferSchema', 'true') \\\n",
    "        .load('./updated_salaries.csv')\n",
    "    print('Salary data loaded successfully')\n",
    "except Exception as e:\n",
    "    print(f'Error loading salary data: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a403c092-bf3f-473a-9c36-d468f6e37baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Show the first few rows of the employees data\n",
    "emp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7fabf1f-f9bf-4554-b65c-48771773c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+----------+\n",
      "|emp_no|salary|dept_no| from_date|   to_date|\n",
      "+------+------+-------+----------+----------+\n",
      "| 10017| 71380|   d001|1993-08-03|1994-08-03|\n",
      "| 10017| 75538|   d001|1994-08-03|1995-08-03|\n",
      "| 10017| 79510|   d001|1995-08-03|1996-08-02|\n",
      "| 10017| 82163|   d001|1996-08-02|1997-08-02|\n",
      "| 10017| 86157|   d001|1997-08-02|1998-08-02|\n",
      "+------+------+-------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Show the first few rows of the salaries data\n",
    "sal_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3950863-3d2f-49ea-bb9a-3586570f535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the schema for employees data\n",
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f45eb2-beec-4e9e-9a1f-33d88c04841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- dept_no: string (nullable = true)\n",
      " |-- from_date: date (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the schema for salary data\n",
    "sal_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812d9ee-e0a8-49f2-b6bc-bbb4ab853818",
   "metadata": {},
   "source": [
    "# Task Three: Clean and process the data\n",
    "In this task, you will perform quick data cleaning by converting variables to proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae18cc0-f1f5-4b72-82fe-51995b4b3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "## Cast the 'emp_no' column in the employees data to a string\n",
    "\n",
    "\n",
    "## Print the updated schema\n",
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8470fe-fa25-45a9-a420-3c877db60a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain transformations to cast 'emp_no' to string and 'to_date' & 'from_date' to date\n",
    "\n",
    "\n",
    "## Show the updated schema\n",
    "sal_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b03eb-41e9-4b5b-9571-42e96b86cb15",
   "metadata": {},
   "source": [
    "# Task Four: Explore the data\n",
    "In this task, you will explore the salaries data by computing summary statistics and visualizing the salary column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9845e-704a-4f06-9d63-530fa0d7d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sum of missing values per column in the salary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55da30-a71a-46c3-9cfb-a94bf36a21ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create the summary statistics for the salary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85285cb5-57f7-48fb-a55f-952981f18b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count total rows and unique employees in salary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fac36-e51e-4da2-a5ad-6ba362709bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the salary distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d48859-39f7-4967-85b6-53c08fa3568f",
   "metadata": {},
   "source": [
    "# Practice Activity One: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6487ce1-b885-4ada-b44e-6fcf3bff48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sum of missing values per column in the employees data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70795dbd-e31d-49b6-9c5b-8797a6fd17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of rows in the employees data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab36f36-0b27-4b94-9464-010b8bd73238",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many different first names can be found in the employees data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f0f7a-2a23-4ac8-b4b3-e16f8e89c312",
   "metadata": {},
   "source": [
    "# Task Five: Aggregate and summarize the data\n",
    "In this task, you will perform data aggregation and summarization using the salaries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebf7cb-6e44-423d-80ff-b2da279c8142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Group the data and calculate the average salary for each department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ae615-4034-4114-82e7-d3f0a9dffe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the average salary and number of employees in each department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7a016-d1f4-41fa-88ba-ae9765bc9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Spark data frame  to Pandas for visualization\n",
    "dept_summary_df = dept_summary.toPandas()\n",
    "\n",
    "## Plot the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f13853-b247-4493-a892-785be265fded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Retrieve a list of employee numbers and the average salary.\n",
    "## Make sure that you return where the average salary is more than $120,000\n",
    "\n",
    "## Group by employee number and calculate the average salary\n",
    "\n",
    "\n",
    "## Order in descending order of average_salary\n",
    "\n",
    "\n",
    "## Show the result\n",
    "emp_avg_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa860eae-8548-415c-bc52-e6ace6028f16",
   "metadata": {},
   "source": [
    "# Task Six: Join the data sets\n",
    "In this task, you will join the salaries and employees data using the employees number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace7be7-7f65-4f3f-a0ab-eabbedb8f025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Create an age column in the employees data\n",
    "## Age when the employee was hired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692b74e-bfb3-4ad4-9917-0627efa3870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join salaries and employees data on 'emp_no'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d3a34-614e-4fb8-bf6c-e2bd2ce551e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve a list of employee numbers and the average salary.\n",
    "## Make sure that you return where the average salary is more than $120,000\n",
    "\n",
    "## Group by employee number and calculate the average salary\n",
    "\n",
    "\n",
    "## Join the aggregated result back with the original employee data to get first_name, last_name, hire_date\n",
    "\n",
    "\n",
    "\n",
    "emp_salary_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a831e-ba55-4e3b-93d8-e7de726d9091",
   "metadata": {},
   "source": [
    "# Cumulative Activity: Analyze employees' retention\n",
    "\n",
    "As a junior data analyst at a growing company, you are tasked with analyzing employee retention. Your aim is to find departments with the highest amount of employees that have worked longer than ten years. This will assist HR in enhancing employees' engagement and retention strategies. \n",
    "\n",
    "To complete this activity, you will use the employee dataset and create a data frame with the employee totals in each department for a period over 10 years (calculated by from_date and to_date).  Finally, you'll visualize how long-term employees are spread across departments via a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d89db-5b94-4f43-9501-e53977ffbc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "## Calculate the years worked based on the difference between 'to_date' and 'from_date'\n",
    "\n",
    "## Group by emp_no and dept_no to sum the years worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade01c7-f7ba-4d51-b00b-3c1c02c51399",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter employees who have worked more than 10 years\n",
    "\n",
    "\n",
    "## Group by department and count distinct employees who worked more than 10 years\n",
    "\n",
    "\n",
    "## Show the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72289f86-e758-4777-bd40-8524ecd38c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the Spark data frame to Pandas for visualization\n",
    "\n",
    "\n",
    "## Create a bar chart to visualize the distribution of long-term employees across departments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ab51f-d191-41d0-b846-214ffea74d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
